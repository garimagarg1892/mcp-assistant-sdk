#!/usr/bin/env node

const { Server } = require("@modelcontextprotocol/sdk/server/index.js");
const {
  StdioServerTransport,
} = require("@modelcontextprotocol/sdk/server/stdio.js");
const {
  CallToolRequestSchema,
  ListToolsRequestSchema,
} = require("@modelcontextprotocol/sdk/types.js");

// Import your existing tools
const createBucket = require("./tools/createBucket");
const deleteBucket = require("./tools/deleteBucket");
const listBuckets = require("./tools/listBuckets");
const putObject = require("./tools/putObject");
const getObject = require("./tools/getObject");
const deleteObject = require("./tools/deleteObject");

// Import new file system tools
const readFile = require("./tools/readFile");
const listDirectory = require("./tools/listDirectory");

// Import MySQL bridge tool
const exportTableToStorage = require("./tools/exportTableToStorage");

// Load environment variables
require("dotenv").config();

class S3MCPServer {
  constructor() {
    this.server = new Server(
      {
        name: "mcp-s3-assistant",
        version: "1.0.0",
      },
      {
        capabilities: {
          tools: {},
        },
      }
    );

    this.setupToolHandlers();
    this.setupErrorHandling();
  }

  setupToolHandlers() {
    // Handle tool listing
    this.server.setRequestHandler(ListToolsRequestSchema, async () => {
      return {
        tools: [
          {
            name: "create_bucket",
            description: `Create an AWS S3 bucket with intelligent parameter extraction from natural language.

ðŸ§  PARAMETER EXTRACTION INTELLIGENCE:
Analyze the user's request and extract relevant AWS S3 parameters. If the user doesn't provide a bucket name, ASK FOR IT.

ðŸ“‹ PARAMETER MAPPING GUIDE:

REQUIRED:
- bucketName: ALWAYS REQUIRED - if not provided, ask the user for it

OPTIONAL PARAMETERS (extract if mentioned):

ðŸ”’ ACCESS CONTROL:
- acl: "private" | "public-read" | "public-read-write" | "authenticated-read"
  â†’ Default: "private" (if no access mentioned)
  â†’ From: "private", "public", "public read", "authenticated", "secure"

ðŸŒ REGION MAPPING:
- locationConstraint: AWS region code
  â†’ Default: "us-east-1" (if no region mentioned)
  â†’ COMMON MAPPINGS:
    â€¢ "Europe", "EU", "Ireland" â†’ "eu-west-1"
    â€¢ "London", "UK" â†’ "eu-west-2" 
    â€¢ "Paris", "France" â†’ "eu-west-3"
    â€¢ "Frankfurt", "Germany" â†’ "eu-central-1"
    â€¢ "Virginia", "US East" â†’ "us-east-1"
    â€¢ "Ohio", "US East 2" â†’ "us-east-2"
    â€¢ "Oregon", "US West" â†’ "us-west-2"
    â€¢ "California", "US West 1" â†’ "us-west-1"
    â€¢ "Asia", "Singapore" â†’ "ap-southeast-1"
    â€¢ "Tokyo", "Japan" â†’ "ap-northeast-1"
    â€¢ "Sydney", "Australia" â†’ "ap-southeast-2"
    â€¢ "Canada", "Toronto" â†’ "ca-central-1"
  â†’ If ambiguous (e.g., just "Asia"), ask for clarification

ðŸ” ADVANCED FEATURES:
- objectLockEnabledForBucket: true | false
  â†’ From: "object lock", "versioning", "immutable", "compliance", "retention"
  
- objectOwnership: "BucketOwnerPreferred" | "ObjectWriter" | "BucketOwnerEnforced"
  â†’ Default: "BucketOwnerEnforced"
  â†’ From: "bucket owner", "writer owns", "enforced ownership"

ðŸŽ¯ EXAMPLES:
"Create a private bucket called 'data-backup'" â†’ { bucketName: "data-backup", acl: "private" }
"Public bucket in Europe" â†’ ASK for bucket name first
"Secure bucket in Tokyo with object lock" â†’ { bucketName: "...", acl: "private", locationConstraint: "ap-northeast-1", objectLockEnabledForBucket: true }
"Create bucket in Asia" â†’ ASK which specific Asian region

âš ï¸ ERROR HANDLING:
- If bucket name missing: "I need a bucket name. What would you like to call this bucket?"
- If region ambiguous: "Which specific region? Available options: [list relevant regions]"
- If request unclear: "Could you clarify [specific ambiguous part]?"

Pass all extracted parameters as a flat object.`,
            inputSchema: {
              type: "object",
              properties: {
                bucketName: {
                  type: "string",
                  description:
                    "Name of the S3 bucket to create (REQUIRED - ask user if not provided)",
                },
              },
              required: ["bucketName"],
              additionalProperties: true,
            },
          },
          {
            name: "delete_bucket",
            description: `Delete an AWS S3 bucket with intelligent parameter extraction from natural language.

ðŸ§  PARAMETER EXTRACTION INTELLIGENCE:
Analyze the user's request and extract relevant AWS S3 parameters. If the user doesn't provide a bucket name, ASK FOR IT.

ðŸ“‹ PARAMETER MAPPING GUIDE:

REQUIRED:
- bucketName: ALWAYS REQUIRED - if not provided, ask the user for it

âš ï¸ IMPORTANT NOTES:
- The bucket must be empty before it can be deleted
- This operation is irreversible
- All bucket policies, ACLs, and configurations will be permanently removed

ðŸŽ¯ EXAMPLES:
"Delete bucket 'old-data'" â†’ { bucketName: "old-data" }
"Remove the test-bucket" â†’ { bucketName: "test-bucket" }
"Delete my-bucket" â†’ { bucketName: "my-bucket" }

âš ï¸ ERROR HANDLING:
- If bucket name missing: "I need a bucket name. Which bucket would you like to delete?"
- If bucket not empty: "The bucket must be empty before it can be deleted"
- If bucket doesn't exist: "The specified bucket does not exist"

Pass all extracted parameters as a flat object.`,
            inputSchema: {
              type: "object",
              properties: {
                bucketName: {
                  type: "string",
                  description:
                    "Name of the S3 bucket to delete (REQUIRED - ask user if not provided)",
                },
              },
              required: ["bucketName"],
              additionalProperties: true,
            },
          },
          {
            name: "list_buckets",
            description: `List S3 buckets with intelligent filtering and pagination.

ðŸ§  PARAMETER EXTRACTION INTELLIGENCE:
Extract filtering and pagination parameters from natural language requests.

ðŸ“‹ PARAMETER MAPPING GUIDE:

OPTIONAL PARAMETERS (extract if mentioned):

ðŸ”¢ PAGINATION:
- maxBuckets: Integer (1-1000)
  â†’ From: "first 10 buckets", "limit to 5", "show 20 buckets"
  â†’ Default: All buckets if not specified

- continuationToken: String
  â†’ From: "continue from token", "next page", pagination context
  â†’ Used for paginated results

ðŸ” FILTERING:
- prefix: String  
  â†’ From: "buckets starting with 'test'", "buckets beginning with 'prod'"
  â†’ Limits response to bucket names that begin with specified prefix

- bucketRegion: String (AWS region code)
  â†’ From: "buckets in Europe", "US buckets", "Asia buckets"
  â†’ REGION MAPPINGS (same as createBucket):
    â€¢ "Europe", "EU" â†’ "eu-west-1"
    â€¢ "Virginia", "US East" â†’ "us-east-1" 
    â€¢ "Asia", "Singapore" â†’ "ap-southeast-1"
    â€¢ "Tokyo", "Japan" â†’ "ap-northeast-1"

ðŸŽ¯ EXAMPLES:
"List all buckets" â†’ {} (no parameters)
"Show first 10 buckets" â†’ { maxBuckets: 10 }
"List buckets starting with 'test'" â†’ { prefix: "test" }
"Show buckets in Europe, limit 5" â†’ { bucketRegion: "eu-west-1", maxBuckets: 5 }
"Buckets beginning with 'prod' in Asia" â†’ { prefix: "prod", bucketRegion: "ap-southeast-1" }

âš ï¸ ERROR HANDLING:
- If region ambiguous: "Which specific region? (eu-west-1, us-east-1, ap-southeast-1, etc.)"
- If maxBuckets invalid: "MaxBuckets must be between 1 and 1000"

Pass all extracted parameters as a flat object.`,
            inputSchema: {
              type: "object",
              properties: {},
              additionalProperties: true,
            },
          },
          {
            name: "put_object",
            description: `Universal upload tool - handles direct content, single files, or entire directories!

ðŸ§  PARAMETER EXTRACTION INTELLIGENCE:
Extract upload parameters from natural language requests. This tool handles 3 scenarios automatically.

ðŸ“‹ PARAMETER MAPPING GUIDE:

REQUIRED:
- bucketName: String - Target bucket name

UPLOAD SCENARIOS (choose one):

ðŸ”¸ SCENARIO 1: Direct Content Upload
- fileName: String - Object key/file name
- fileContent: String - Raw content (base64 for binary)

ðŸ”¸ SCENARIO 2: Single File Upload  
- filePath: String - Path to local file to upload
- s3Key: String (optional) - Custom S3 object key

ðŸ”¸ SCENARIO 3: Directory Upload (Batch)
- directoryPath: String - Path to directory to upload
- preserveStructure: Boolean - Keep folder structure (default: true)
- s3Prefix: String (optional) - Prefix for all objects
- fileExtensions: Array (optional) - Filter by extensions like [".jpg", ".pdf"]
- maxFiles: Integer (optional) - Limit files uploaded (default: 100)

OPTIONAL PARAMETERS (all scenarios):
- acl: "private" | "public-read" | "public-read-write" | "authenticated-read"
- serverSideEncryption: "AES256" | "aws:kms" | "aws:kms:dsse"
- storageClass: "STANDARD" | "STANDARD_IA" | "GLACIER" | "DEEP_ARCHIVE"
- contentType: String - MIME type (auto-detected for files)
- metadata: Object - Custom metadata key-value pairs

ðŸŽ¯ EXAMPLES:
"Upload hello.txt with content 'Hello World'" â†’ { bucketName: "my-bucket", fileName: "hello.txt", fileContent: "Hello World" }
"Upload /home/user/photo.jpg to my-bucket" â†’ { bucketName: "my-bucket", filePath: "/home/user/photo.jpg" }
"Upload entire photos directory with backup/ prefix" â†’ { bucketName: "my-bucket", directoryPath: "/home/user/photos", s3Prefix: "backup" }
"Upload only PDFs from documents folder" â†’ { bucketName: "my-bucket", directoryPath: "/home/user/docs", fileExtensions: [".pdf"] }

âœ¨ SMART FEATURES:
- Automatic file type detection (binary vs text)
- Content-Type auto-detection from file extensions
- Directory structure preservation or flattening
- Batch upload progress tracking
- Comprehensive error handling

âš ï¸ ERROR HANDLING:
- Missing bucket: "bucketName is required"
- Invalid scenario: "Provide either fileContent+fileName, filePath, or directoryPath"
- File not found: Clear error with file path
- Batch failures: Detailed per-file error reporting

Pass all extracted parameters as a flat object.`,
            inputSchema: {
              type: "object",
              properties: {
                bucketName: {
                  type: "string",
                  description: "Target bucket name",
                },
                fileName: {
                  type: "string",
                  description: "Object key/file name (for direct content)",
                },
                fileContent: {
                  type: "string",
                  description: "File content (for direct content)",
                },
                filePath: {
                  type: "string",
                  description: "Local file path to upload",
                },
                directoryPath: {
                  type: "string",
                  description: "Local directory path to upload",
                },
                s3Key: { type: "string", description: "Custom S3 object key" },
                preserveStructure: {
                  type: "boolean",
                  description: "Keep directory structure in S3",
                },
                s3Prefix: {
                  type: "string",
                  description: "Prefix for uploaded objects",
                },
                fileExtensions: {
                  type: "array",
                  items: { type: "string" },
                  description: "Filter by file extensions",
                },
                maxFiles: {
                  type: "integer",
                  description: "Maximum files to upload",
                },
              },
              required: ["bucketName"],
              additionalProperties: true,
            },
          },
          {
            name: "get_object",
            description: `Download an object from S3 bucket with advanced options.

ðŸ§  PARAMETER EXTRACTION INTELLIGENCE:
Extract download parameters and conditions from natural language requests.

ðŸ“‹ PARAMETER MAPPING GUIDE:

REQUIRED:
- bucketName: String - Source bucket name
- fileName: String - Object key/file name to download

OPTIONAL PARAMETERS (extract if mentioned):

ðŸ”„ CONDITIONAL DOWNLOADS:
- ifMatch: String (ETag)
  â†’ From: "if ETag matches", "only if unchanged"
  
- ifModifiedSince: Date
  â†’ From: "if modified since", "updated after", "newer than"
  
- ifNoneMatch: String (ETag) 
  â†’ From: "if different", "if changed", "if ETag differs"

ðŸ“ RANGE REQUESTS:
- range: String
  â†’ From: "first 1000 bytes", "bytes 0-1023", "partial download"
  â†’ Format: "bytes=0-1023"

ðŸ” ENCRYPTION:
- sseCustomerAlgorithm: String
  â†’ From: "customer encryption", "SSE-C", "AES256"
  
- versionId: String
  â†’ From: "specific version", "version ID", "historical version"

ðŸ“‹ RESPONSE MODIFICATION:
- responseCacheControl: String
  â†’ From: "cache control", "no-cache", "max-age"
  
- responseContentType: String
  â†’ From: "return as", "content type", "MIME type override"

ðŸŽ¯ EXAMPLES:
"Download file.txt from bucket" â†’ { bucketName: "...", fileName: "file.txt" }
"Get first 1000 bytes of data.bin" â†’ { ..., range: "bytes=0-999" }
"Download if modified since yesterday" â†’ { ..., ifModifiedSince: "2024-01-01" }
"Get specific version of document" â†’ { ..., versionId: "abc123" }

âš ï¸ ERROR HANDLING:
- Missing required params: "Need bucketName and fileName"
- Invalid range format: "Range must be 'bytes=start-end'"

Pass all extracted parameters as a flat object.`,
            inputSchema: {
              type: "object",
              properties: {
                bucketName: {
                  type: "string",
                  description: "Source bucket name",
                },
                fileName: {
                  type: "string",
                  description: "Object key/file name to download",
                },
              },
              required: ["bucketName", "fileName"],
              additionalProperties: true,
            },
          },
          {
            name: "read_file",
            description: `Read any file from the local file system with automatic type detection.

ðŸ§  PARAMETER EXTRACTION INTELLIGENCE:
Extract file reading parameters from natural language requests.

ðŸ“‹ PARAMETER MAPPING GUIDE:

REQUIRED:
- filePath: String - Path to the file to read

ðŸŽ¯ EXAMPLES:
"Read the file /home/user/document.pdf" â†’ { filePath: "/home/user/document.pdf" }
          {
            name: "delete_object",
            description: `Delete an object from S3 bucket with versioning and governance support.

ðŸ§  PARAMETER EXTRACTION INTELLIGENCE:
Extract deletion parameters from natural language requests.

ðŸ“‹ PARAMETER MAPPING GUIDE:

REQUIRED:
- bucketName: String - Source bucket name
- fileName: String - Object key/file name to delete

OPTIONAL PARAMETERS (extract if mentioned):

ðŸ”„ VERSIONING:
- versionId: String
  â†’ From: "specific version", "version ID", "delete version"
  â†’ Permanently deletes a specific version

ðŸ” ADVANCED OPTIONS:
- mfa: String
  â†’ From: "with MFA", "MFA delete"
  â†’ Format: "serial-number token-code"
  â†’ Required for MFA Delete enabled buckets

- bypassGovernanceRetention: Boolean
  â†’ From: "bypass governance", "override retention"
  â†’ Requires s3:BypassGovernanceRetention permission

- requestPayer: String
  â†’ From: "requester pays"
  â†’ Possible values: "requester"

- expectedBucketOwner: String
  â†’ From: "expected owner", "account ID"
  â†’ Account ID validation

ðŸŽ¯ EXAMPLES:
"Delete file.txt from bucket" â†’ { bucketName: "...", fileName: "file.txt" }
"Delete specific version of document" â†’ { ..., versionId: "abc123" }
"Delete with MFA" â†’ { ..., mfa: "serial-number token" }
"Delete bypassing governance" â†’ { ..., bypassGovernanceRetention: true }

âš ï¸ IMPORTANT NOTES:
- S3 delete is idempotent - success doesn't guarantee object existed
- Versioned buckets create delete markers instead of permanent deletion
- Use versionId to permanently delete a specific version
- MFA delete requires HTTPS and proper MFA configuration

ðŸ›¡ï¸ PERMISSIONS REQUIRED:
- s3:DeleteObject - Standard delete permission
- s3:DeleteObjectVersion - For deleting specific versions
- s3:BypassGovernanceRetention - For bypassing governance mode

Pass all extracted parameters as a flat object.`,
            inputSchema: {
              type: "object",
              properties: {
                bucketName: {
                  type: "string",
                  description: "Source bucket name",
                },
                fileName: {
                  type: "string",
                  description: "Object key/file name to delete",
                },
                versionId: {
                  type: "string",
                  description: "Specific version ID to delete",
                },
                mfa: {
                  type: "string",
                  description: "MFA authentication (serial-number token-code)",
                },
                bypassGovernanceRetention: {
                  type: "boolean",
                  description: "Bypass governance retention mode",
                },
              },
              required: ["bucketName", "fileName"],
              additionalProperties: true,
            },
          },
"Show me the contents of config.json" â†’ { filePath: "config.json" }
"What's in that image file?" â†’ { filePath: "image.jpg" }

âœ¨ FEATURES:
- Automatic binary/text detection
- Proper encoding handling (base64 for binary, utf8 for text)
- Content type detection from file extension
- File metadata (size, modified date, etc.)
- Support for all file types (images, documents, code, etc.)

âš ï¸ ERROR HANDLING:
- File not found: Clear error message with file path
- Permission denied: Helpful access error information
- Invalid path: Path validation and suggestions`,
            inputSchema: {
              type: "object",
              properties: {
                filePath: {
                  type: "string",
                  description: "Path to the file to read",
                },
              },
              required: ["filePath"],
              additionalProperties: true,
            },
          },
          {
            name: "list_directory",
            description: `List contents of a directory with filtering and browsing options.

ðŸ§  PARAMETER EXTRACTION INTELLIGENCE:
Extract directory listing parameters from natural language requests.

ðŸ“‹ PARAMETER MAPPING GUIDE:

REQUIRED:
- directoryPath: String - Path to the directory to list

OPTIONAL PARAMETERS:
- showHidden: Boolean - Show hidden files/folders (default: false)
- filesOnly: Boolean - Show only files (default: false)
- dirsOnly: Boolean - Show only directories (default: false)
- recursive: Boolean - Recursive listing (default: false)
- maxDepth: Integer - Maximum depth for recursive listing (default: 3)

ðŸŽ¯ EXAMPLES:
"List files in /home/user/documents" â†’ { directoryPath: "/home/user/documents" }
"Show all files including hidden ones" â†’ { directoryPath: ".", showHidden: true }
"List only directories recursively" â†’ { directoryPath: "/path", dirsOnly: true, recursive: true }
"Browse folder structure 2 levels deep" â†’ { directoryPath: "/path", recursive: true, maxDepth: 2 }

âœ¨ FEATURES:
- File and directory information (size, modified date, type)
- Filtering options (files only, directories only, hidden files)
- Recursive directory traversal
- Sorted output (directories first, then files)
- File extension detection

âš ï¸ ERROR HANDLING:
- Directory not found: Clear error with path
- Permission denied: Access error information
- Invalid path: Path validation`,
            inputSchema: {
              type: "object",
              properties: {
                directoryPath: {
                  type: "string",
                  description: "Path to the directory to list",
                },
                showHidden: {
                  type: "boolean",
                  description: "Show hidden files and folders",
                },
                filesOnly: { type: "boolean", description: "Show only files" },
                dirsOnly: {
                  type: "boolean",
                  description: "Show only directories",
                },
                recursive: { type: "boolean", description: "List recursively" },
                maxDepth: {
                  type: "integer",
                  description: "Maximum depth for recursive listing",
                },
              },
              required: ["directoryPath"],
              additionalProperties: true,
            },
          },
          {
            name: "export_table_to_storage",
            description: `Export MySQL table data to S3/ObjectStore via MCP bridge.

ðŸŒ‰ MCP BRIDGE INTELLIGENCE:
This tool connects MySQL MCP with your S3 MCP to create a seamless data pipeline.

ðŸ“‹ PARAMETER MAPPING GUIDE:

REQUIRED:
- tableName: String - Name of the MySQL table to export
- bucketName: String - Target S3 bucket name

MYSQL CONNECTION (optional - uses env vars if not provided):
- mysqlHost: String - MySQL server host (default: localhost)
- mysqlPort: Integer - MySQL server port (default: 3306)  
- mysqlUser: String - MySQL username (default: root)
- mysqlPassword: String - MySQL password
- mysqlDatabase: String - MySQL database name

QUERY OPTIONS (optional):
- where: String - WHERE clause for filtering data
- orderBy: String - ORDER BY clause for sorting
- limit: Integer - LIMIT number of records

OUTPUT OPTIONS (optional):
- format: "json" | "csv" | "jsonl" (default: json)
- fileName: String - Custom output filename
- prettyFormat: Boolean - Pretty print JSON (default: true)
- csvDelimiter: String - CSV delimiter (default: comma)

S3 OPTIONS (optional - same as putObject):
- s3Prefix: String - S3 object prefix
- acl: String - S3 access control
- serverSideEncryption: String - Encryption type
- storageClass: String - S3 storage class
- metadata: Object - Custom metadata

ðŸŽ¯ EXAMPLES:
"Export users table to my-bucket" â†’ { tableName: "users", bucketName: "my-bucket" }
"Export products as CSV to backup-bucket" â†’ { tableName: "products", bucketName: "backup-bucket", format: "csv" }
"Export recent orders to archive" â†’ { tableName: "orders", bucketName: "archive", where: "created_date > '2024-01-01'" }

âœ¨ FEATURES:
- Automatic MySQL MCP communication
- Multiple export formats (JSON, CSV, JSONL)
- Query filtering and sorting
- Seamless S3 upload via existing putObject
- Comprehensive metadata tracking

âš ï¸ REQUIREMENTS:
- MySQL MCP server must be installed and accessible
- Valid MySQL connection parameters (via env or params)
- Target S3 bucket must exist

Pass all extracted parameters as a flat object.`,
            inputSchema: {
              type: "object",
              properties: {
                tableName: {
                  type: "string",
                  description: "MySQL table name to export",
                },
                bucketName: {
                  type: "string",
                  description: "Target S3 bucket name",
                },
                mysqlHost: { type: "string", description: "MySQL server host" },
                mysqlPort: {
                  type: "integer",
                  description: "MySQL server port",
                },
                mysqlUser: { type: "string", description: "MySQL username" },
                mysqlPassword: {
                  type: "string",
                  description: "MySQL password",
                },
                mysqlDatabase: {
                  type: "string",
                  description: "MySQL database name",
                },
                where: {
                  type: "string",
                  description: "WHERE clause for filtering",
                },
                orderBy: { type: "string", description: "ORDER BY clause" },
                limit: {
                  type: "integer",
                  description: "LIMIT number of records",
                },
                format: {
                  type: "string",
                  description: "Output format (json, csv, jsonl)",
                },
                fileName: {
                  type: "string",
                  description: "Custom output filename",
                },
                s3Prefix: { type: "string", description: "S3 object prefix" },
              },
              required: ["tableName", "bucketName"],
              additionalProperties: true,
            },
          },
        ],
      };
    });

    // Handle tool execution
    this.server.setRequestHandler(CallToolRequestSchema, async (request) => {
      const { name, arguments: args } = request.params;

      try {
        let result;

        switch (name) {
          case "create_bucket":
            // Pass ALL arguments directly to the tool - truly generic!
            result = await createBucket(args);
            break;

          case "delete_bucket":
            // Pass ALL arguments directly to the tool - truly generic!
            result = await deleteBucket(args);
            break;

          case "list_buckets":
            // Pass ALL arguments directly to the tool - truly generic!
            result = await listBuckets(args);
            break;

          case "put_object":
            // Pass ALL arguments directly to the tool - truly generic!
            result = await putObject(args);
            break;

          case "get_object":
            // Pass ALL arguments directly to the tool - truly generic!
            result = await getObject(args);
            break;


          case "delete_object":
            // Pass ALL arguments directly to the tool - truly generic!
            result = await deleteObject(args);
            break;
          case "read_file":
            // File system tool - read any file type
            result = await readFile(args);
            break;

          case "list_directory":
            // File system tool - browse directories
            result = await listDirectory(args);
            break;

          case "export_table_to_storage":
            // MySQL MCP bridge tool - export table data to S3
            result = await exportTableToStorage(args);
            break;

          default:
            throw new Error(`Unknown tool: ${name}`);
        }

        return {
          content: [
            {
              type: "text",
              text: JSON.stringify(result, null, 2),
            },
          ],
        };
      } catch (error) {
        return {
          content: [
            {
              type: "text",
              text: `Error executing ${name}: ${error.message}`,
            },
          ],
          isError: true,
        };
      }
    });
  }

  setupErrorHandling() {
    this.server.onerror = (error) => {
      console.error("[MCP Server Error]", error);
    };

    process.on("SIGINT", async () => {
      await this.server.close();
      process.exit(0);
    });
  }

  async run() {
    const transport = new StdioServerTransport();
    await this.server.connect(transport);
    console.error("MCP S3 Assistant Server running on stdio");
  }
}

// Start the server
if (require.main === module) {
  const server = new S3MCPServer();
  server.run().catch((error) => {
    console.error("Failed to start server:", error);
    process.exit(1);
  });
}

module.exports = S3MCPServer;
